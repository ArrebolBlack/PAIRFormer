# @package _global_


# CheapCTSNet.yaml
# 一份“单实验”的完整配置，可以直接作为 config_name 使用
# 或者放在 configs/experiment/ 里当 baseline 模板。

# configs/experiment/CheapCTSNet.yaml


# ---- 顶层元信息 ----
experiment_name: "CheapCTSNet"  # 实验名，用于 WandB / 日志前缀
experiment:
  name: CheapCTSNet.yaml
  task: window_level_train    

seed: 2020                                   # 全局随机种子 
device: "cuda"                               # "cuda" / "cpu"

# ============================================================
# 1. 数据配置 data
# ============================================================
data:
  # 数据集名字（纯标识，用于日志）
  name: mirna_miraw

  # 原始 txt 路径（相对于项目根目录）
  path:
    train: "data/miRAW_Train_Validation.txt"
    val:   "data/miRAW_Train_Validation.txt"
    test:  "data/miRAW_Test_total.txt"

  # 是否包含 ESA 特征（保持与你现有 DataConfig 逻辑一致）
  with_esa: true

# ============================================================
# 2. 模型配置 model
# ============================================================
model:
  # 模型在实验中的名字（随便起，用于日志）
  name: CheapCTSNet

  # 模型架构标识：用于选择具体实现
  arch: CheapCTSNet_TinyConv
  # ---- 消融 ----- # 
  # 1. 纯内容（No meta）
  # meta_mode: none
  # 2. 主线推荐（embedding 不看 meta；logit 看 meta）
  meta_mode: logit_only
  meta_dropout: 0.2   # 可选：防捷径
  # 3. meta 同时进 emb 和 logit（用于消融）
  # meta_mode: emb_and_logit

  # 模型结构超参
  emb_dim: 64
  use_diff: true
  dropout: 0.0
  logit_hidden_dim: 64 # logit head 中间层dim，<= 0 则直接映射到logit（无中间层）
  # TinyConv 主干 channels, kernel_size, stride
  c1: 16
  c2: 32
  k1: 5
  k2: 3
  s1: 2
  s2: 2



# ============================================================
# 3. 训练配置 train（Trainer 用）
# ============================================================
train:

#####################
  # --- 优化器 & 学习率 ---
  optimizer: adamw              # ["adamw", "adam", "sgd", "rmsprop"]
  # lr: 0.005
  lr: 0.006
  weight_decay: 0.0
  momentum: 0.9                # 仅对 SGD 有效

  # --- 学习率调度器 ---
  scheduler: cosine              # ["none", "plateau", "cosine", "step"]
  scheduler_factor: 0.2        # for ReduceLROnPlateau
  scheduler_patience: 5
  scheduler_t_max: 100          # for CosineAnnealingLR
  scheduler_step_size: 10      # for StepLR
  scheduler_gamma: 0.1

  # --- 损失 & 数值稳定 ---
  # === Stage 0.1: class weighting + label smoothing ===
  pos_weight: 1.0       # 1.0 = 不加权
  label_smoothing: true
  smooth_pos: 0.95       # label_smoothing=false 时不生效
  smooth_neg: 0.05
  # === Stage 0.2: Focal Loss ===
  loss_type: bce_focal          # ["bce", "focal", "bce_focal"]
  focal_gamma: 1.0
  focal_alpha: 0.4
  focal_lambda: 1
  bce_lambda: 0.01 # bce_loss 数值一般在focal_loss的十倍
  # bce_focal = bce_lambda * bce_loss + focal_lambda * focal_loss
  # === Stage 0.3: esa weight(window-size) ===
  esa_weighting: true
  esa_scale: 10.0     # ESA 归一化尺度，esa_norm = clamp(score / esa_scale, 0..1)
  esa_lambda_pos: 1.0 # 正样本 ESA 权重系数
  esa_lambda_neg: 0.5 # 负样本 ESA 权重系数（一般从 0 或 0.5 小试）

  amp: true                    # 是否使用混合精度
  grad_clip: 1.0               # None 或 float

  # --- EMA（可选） ---
  ema:
    enabled: true
    decay: 0.999
######################


  # # --- 优化器 & 学习率 ---
  # optimizer: adam              # ["adamw", "adam", "sgd", "rmsprop"]
  # lr: 0.001
  # weight_decay: 0
  # momentum: 0.9                # 仅对 SGD 有效

  # # --- 学习率调度器 ---
  # scheduler: plateau              # ["none", "plateau", "cosine", "step"]
  # scheduler_factor: 0.2        # for ReduceLROnPlateau
  # scheduler_patience: 5
  # scheduler_t_max: 10          # for CosineAnnealingLR
  # scheduler_step_size: 10      # for StepLR
  # scheduler_gamma: 0.1

  # # --- 损失 & 数值稳定 ---
  # loss_type: bce               # "bce" or "mse" or "custom"
  # amp: false                    # 是否使用混合精度
  # grad_clip: 1.0               # None 或 float

  # # --- EMA（可选） ---
  # ema:
  #   enabled: false
  #   decay: 0.999

  # --- 监控指标（决定 best.pt 保存逻辑） ---
  monitor: loss                # "loss" / "f1" / "roc_auc" / ...
  greater_is_better: false     # 对 loss 应设为 false，对 F1/AUC 设为 true

# ============================================================
# 4. 任务配置 task（metrics / evaluator 用）
# ============================================================
task:
  # 问题类型：影响损失与 metrics 行为
  problem_type: binary_classification   # or "regression"

  # y_pred_raw 是否为 logits，需要内部做 sigmoid
  from_logits: true

  # 最简单版本：固定阈值
  # （之后可以扩展为 threshold.fixed / sweep 等更复杂结构）
  threshold: 0.5

  ranking:
    enabled: true
    ks: [1, 10, 32, 64, 128]
    compute_topk_overlap: true
    bucket:
      strategy: quantile     # quantile | fixed | null
      num_buckets: 5
      # fixed_edges: [1, 3, 6, 11, 21, 1000000000]


# ============================================================
# 5. 运行配置 run（train.py / eval.py 通用）
# ============================================================
run:
  # 当前运行模式（主要语义标记）
  mode: train                       # "train" / "eval"

  # 训练轮数
  num_epochs: 100

  # DataLoader 超参数（train & eval 共用）
  batch_size: 256
  num_workers: 4
  pin_memory: true

  # 数据缓存路径（通常相对于项目根目录）
  cache_path: "${paths.cache_root}"

  # checkpoint / resume 策略
  resume: false                     # train: 是否尝试从 checkpoint 恢复
  checkpoint: null                  # train 默认 None；eval 模式下必须显式指定

  # 输出子目录（相对于 hydra.run.dir）
  ckpt_subdir: "checkpoints"
  eval_subdir: "eval"
  
  eval_test_after_train: true
  test_splits: ["test"]

  eval_test_with_last: true 
  eval_test_with_best: false 



  eval_collect_teacher_logits: true

  # distill_feat_mode: teacher_proj_trainable  # teacher_proj_frozen, student_proj
  distill_feat_mode: student_proj



  distill_teacher_feat_dim: 384
  distill_student_emb_dim: 64

  distill_alpha_schedule: cosine   # or linear

  distill_T: 2.0
  # distill_alpha_start: 1
  # distill_alpha_end: 0
  # distill_beta_kd: 0.1
  # distill_beta_feat: 0.5
  # distill_beta_rel: 1
  distill_alpha_start: 0.8
  distill_alpha_end: 0.5
  distill_beta_kd: 1
  distill_beta_feat: 0.1
  distill_beta_rel: 1

  distill_rel_max_b: 256

  train_reduction: none   # 关键：禁止 MIL 聚合

  distill_enabled: true
  distill_need_feat: true  # 只要要用 L_rel 或 L_feat，就必须保证：run_cfg.distill_need_feat=true
  distill_teacher_amp: true

  distill_teacher_arch: TargetNet_Optimized
  distill_teacher_ckpt: checkpoints/miRAW_TargetNet_Optimized_dp-0.1/checkpoints/last.pt

  # teacher 的模型超参（与 teacher 当初训练时一致）
  distill_teacher_model:

    name: targetnet_optimized
    arch: TargetNet_Optimized

    num_channels:      [16, 16, 32, 32]
    num_blocks:        [1, 1, 1, 1]
    pool_size:         3
    stem_kernel_size:  5
    block_kernel_size: 3
    skip_connection:   true
    dropout:           0.5

    multi_scale:       false          # 先关掉 multi-scale
    se_type:           "basic"         # 完全关掉 SE
    use_bn:            false          # 先完全关掉 BN
    se_reduction:      8              # 稍微小一点，让 SE 不那么「硬」
    target_output_length: 12          # 对齐原模型池化后的长度

    arch_variant:      "opt4_tiny"


# ============================================================
# 6. 评估配置 eval（evaluator 使用）
# ============================================================
eval:
  # 顶层输出目录名（一般不用改，实际路径= run.eval_subdir / split_idx）
  save_dir: "eval_outputs"

  # 是否在当前 split 上扫阈值（训练结束的 val eval 就会用到）
  do_threshold_sweep: true
  sweep_num_thresholds: 101    # 扫描 0~1 上的阈值个数

  # 是否从 val 阶段的 best_threshold.json 读阈值（主要给 test eval 用）
  use_val_best_threshold: false
  best_threshold_path: null    # 为空时默认尝试 eval/val/best_threshold.json

  # 输出内容控制
  save_metrics_json: true
  save_report_txt: true
  save_threshold_csv: true
  save_curves_png: true

  # 曲线文件名（放在对应 split 目录下）
  roc_curve_file: "roc_curve.png"
  pr_curve_file: "pr_curve.png"

# ============================================================
# 7. 日志 & WandB 配置 logging
# ============================================================
logging:
  # 训练 & eval 统一使用的 WandB 基本设置
  wandb:
    enabled: true                  # 打开后 train/eval 都会 init wandb
    project: "CheapCTSNet"
    entity: null
    mode: "online"                  # "online" / "offline" / "disabled"
    group: miRAW_baseline
    tags: ["CheapCTSNet"]

  # evaluator 内部使用的前缀设置（写 summary 时可以加上 eval/）
  eval:
    use_wandb: false                # 是否在 evaluator 中额外写 wandb summary
    wandb_prefix: "eval"

# ============================================================
# 8. 路径命名空间 paths（统一收口）
# ============================================================
paths:
  # 所有实验输出的根目录（通常配合 Hydra 的 hydra.run.dir 一起用）
  output_root: "outputs"

  # 数据 cache 根目录
  cache_root: "cache"

  # 日志根目录（给以后 logger / 自定义文件日志预留）
  logs_root: "logs"
