# @package _global_


# configs/experiment/miRAW_Transformer.yaml

# 继承原始 TargetNet baseline 的数据 & 训练大部分设置
defaults:
  - miRAW_TargetNet_baseline
  - _self_

experiment_name: "miRAW_Transformer"

data:
  path:
    train: "data/miRAW_Train_Validation.txt"
    val:   "data/miRAW_Train_Validation.txt"
    test: "data/miRAW_Test_total.txt"

model:
  arch: TargetNetTransformer1D         # 使用新注册的模型
  name: targetnet_transformer_baseline

  # === Transformer 结构超参（从你之前的 MyTransformer baseline 迁移而来） ===
  d_model: 128
  n_layers: 2
  n_heads: 4
  dim_ff: 512
  dropout: 0.1

  # 位置编码 & 序列长度相关
  max_len: 64               # 不写则默认 40/50，这里显式给个稍宽裕的
  ff_activation: swiglu       #gelu / relu / swiglu 对应 TransformerConfig.ff_activation
  pos_encoding_type: sinusoidal   # ["none", "sinusoidal", "rope"]
  causal: false             # encoder，用不到因果 mask

  # 可选：如果想明确定义（否则默认 True）
  # with_esa: true


train:
  # --- 优化器 & 学习率 ---
  optimizer: rmsprop            # ["adamw", "adam", "sgd", "rmsprop"]
  lr: 0.001
  weight_decay: 0.0
  momentum: 0.9                # 仅对 SGD 有效

  # --- 学习率调度器 ---
  scheduler: cosine              # ["none", "plateau", "cosine", "step"]
  scheduler_factor: 0.2        # for ReduceLROnPlateau
  scheduler_patience: 5
  scheduler_t_max: 10          # for CosineAnnealingLR
  scheduler_step_size: 10      # for StepLR
  scheduler_gamma: 0.1

  # --- 损失 & 数值稳定 ---
  # === Stage 0.1: class weighting + label smoothing ===
  pos_weight: 1.0       # 1.0 = 不加权
  label_smoothing: true
  smooth_pos: 0.95       # label_smoothing=false 时不生效
  smooth_neg: 0.05
  # === Stage 0.2: Focal Loss ===
  loss_type: bce_focal          # ["bce", "focal", "bce_focal"]
  focal_gamma: 1.0
  focal_alpha: 0.4
  focal_lambda: 1
  bce_lambda: 0.01 # bce_loss 数值一般在focal_loss的十倍
  # bce_focal = bce_lambda * bce_loss + focal_lambda * focal_loss
  # === Stage 0.3: esa weight(window-size) ===
  esa_weighting: true
  esa_scale: 10.0     # ESA 归一化尺度，esa_norm = clamp(score / esa_scale, 0..1)
  esa_lambda_pos: 1.0 # 正样本 ESA 权重系数
  esa_lambda_neg: 0.5 # 负样本 ESA 权重系数（一般从 0 或 0.5 小试）
  



  amp: true                    # 是否使用混合精度
  grad_clip: 1.0               # None 或 float

  # --- EMA（可选） ---
  ema:
    enabled: true
    decay: 0.999

  # --- 监控指标（决定 best.pt 保存逻辑） ---
  monitor: loss                # "loss" / "f1" / "roc_auc" / ...
  greater_is_better: false     # 对 loss 应设为 false，对 F1/AUC 设为 true

run:
  
  num_epochs: 60
  batch_size: 128


# ============================================================
# 7. 日志 & WandB 配置 logging
# ============================================================
logging:
  # 训练 & eval 统一使用的 WandB 基本设置
  wandb:
    enabled: true                  # 打开后 train/eval 都会 init wandb
    project: "TargetNet"
    entity: null
    mode: "online"                  # "online" / "offline" / "disabled"
    group: miRAW_Transformer_final
    tags: ["Transformer"]

  # evaluator 内部使用的前缀设置（写 summary 时可以加上 eval/）
  eval:
    use_wandb: false                # 是否在 evaluator 中额外写 wandb summary
    wandb_prefix: "eval"