# @package _global_
# configs/experiment/miRAW_EM_Pipeline.yaml
# New EM pipeline (Stage-1/Stage-2 with caches): train_em.py + TokenProvider + UpdatePolicy + TrainerEM

# ===================== 实验元信息 =====================
experiment_name: "miRAW_EM_Pipeline"
experiment:
  name: "miRAW_EM_Pipeline"
  task: "em_pair_level_train"   # 仅作标识；train_em.py 不依赖这个字段也能跑

seed: 2020
device: "cuda"

# ===================== 路径命名空间 paths =====================
paths:
  output_root: "outputs"
  cache_root: "cache"
  logs_root: "logs"

# ===================== 数据配置 data（沿用旧 DataConfig 语义） =====================
data:
  name: mirna_miraw
  path:
    # train: "data/miRAW_Train_Validation.txt"
    # val:   "data/miRAW_Train_Validation.txt"
    # test:  "data/miRAW_Test_total.txt"
    train: "data/miRAW_Test1-5_split-ratio-0.9_Train_Validation.txt"
    val:   "data/miRAW_Test1-5_split-ratio-0.9_Train_Validation.txt"
    test: "data/miRAW_Test_0,6-9.txt"
  with_esa: true

# ===================== 运行配置 run（train_em.py 会用到的字段） =====================
run:
  mode: train

  # --- split 控制（train_em.py 使用 split/val_split 构造 ChunkedCTSDataset） ---
  split: "train"
  val_split: "val"

  # --- dataloader ---
  batch_size: 256
  num_workers: 8
  pin_memory: true
  num_epochs: 100

  # --- token budget ---
  kmax: 512

  # --- PairBatchBuilderCPU 可选字段（影响 batch_cpu 是否带 pos/esa_scores） ---
  include_pos: true
  include_esa: true

  # --- cache 根目录（ChunkedCTSDataset 使用）---
  cache_path: "${paths.cache_root}"

  # --- checkpoint / eval 输出子目录（train_em.py 会构造 ckpt_dir/eval_dir） ---
  ckpt_subdir: "checkpoints"
  eval_subdir: "eval"
  ckpt_dir: "${hydra:run.dir}/${run.ckpt_subdir}"
  eval_dir: "${hydra:run.dir}/${run.eval_subdir}"

  resume: false
  checkpoint: null

  # （可选）训练完是否跑 test：train_em.py 当前未实现自动 test，这里先保留占位
  eval_test_after_train: true
  test_splits: ["test"]
  eval_test_with_last: true
  eval_test_with_best: false
  best_ckpt_path: null            # 可选；默认用 ckpt_dir/best.pt

  # MIL 聚合（pair-level 这边不再需要 bag-level MIL）
  train_reduction: none       # Stage-1: window-level TargetNet 的 bag 聚合方式（这里不启用）
  eval_reduction: none        # pair-level 一般就 none；如果你还想对 pair 再做 max，可以改成 max
  test_reduction: none

  # softmax 温度 / topk（如果后面 eval/test 想做 top-k 聚合可以继续用）
  train_topk: 3
  eval_topk: 3
  test_topk: 3
  train_softmax_temp: 1.0
  eval_softmax_temp: 1.0
  test_softmax_temp: 1.0


# ===================== EM 相关（Controller / E-step 离线刷新触发） =====================

em:
  # 单一真源：直接引用 token_provider.policy，避免双份 policy 漂移
  update_policy: ${token_provider.policy}

  controller:
    refresh_on_rank0_only: true
    barrier_after_refresh: true
    rebuild_train_loader_after_selection_refresh: true
    verbose: true

  # （可选）一些 runner 相关参数如果你想集中放 em 下也可以：
  inst_emb_dim: 384
  inst_version: "inst_v0"
  cheap_emb_dim: 64
  cheap_version: "cheap_v0"

  bootstrap:
    enabled: true
    overwrite_all: true     # 强烈建议 True
    # overwrite_all: false  # 如果你要省时间才改 false
    # skip_if_ready: true   # overwrite_all=false 时建议 true
    epoch: 0


  # 1) selector module（Hydra 可 instantiate）
  selector_module:
    _target_: src.selectors.selector_module.SelectorModule
    cfg:
      _target_: src.selectors.st_selector.STSelectorConfig
      kmax: ${run.kmax}               # 保持和 run.kmax 一致，避免 shape 不一致
      k1_ratio: 1
      score_use_sigmoid: false
      exploration_sigma: 0.0
      base_seed: ${seed}
      pos_bin_eps: 1e-6
      use_hash_dedup: true
      hash_seed: ${seed}
      score_norm_z: true
      score_norm_eps: 1e-6
      quota_top_t: 8
      quota_tau_w: 1.0

  # 2) sel_version 建议留空/auto：由 train_em 自动算 hash（见下面代码修复）
  sel_version: null

  # 3) selection cache 的 runner 参数
  selection_cache:
    pair_batch_size: 256



# ===================== EM cache root（实例/selection/cheap 的 memmap 根目录） =====================
# train_em.py 默认 em_cache_root=cache_root；这里显式给出，避免混淆
em_cache_root: "${paths.cache_root}"

# ===================== Cheap 预训练 ckpt（可选） =====================
# train_em.py 会尝试加载；如果为空则从随机/当前权重开始
cheap_ckpt_path: "checkpoints/CheapCTSNet/checkpoints/last.pt"

# ===================== Instance 预训练 ckpt（可选） =====================
# train_em.py 会尝试加载；如果为空则从随机/当前权重开始
instance_ckpt_path: "checkpoints/miRAW_TargetNet_Optimized_dp-0.1/checkpoints/last.pt"

# ===================== TokenProvider（新 pipeline 核心开关） =====================
token_provider:
  # cache miss 处理：Gate 6 推荐 error（fail-fast）
  cache_missing: "error"     # ["error", "online_fallback", "zero"]

  use_amp: false
  normalize_tokens: false

  # token 拼接策略：token = [inst_emb, inst_logit, esa, pos]
  assemble:
    use_inst_emb: true
    use_inst_logit: true
    use_esa: true
    use_pos: true

  # UpdatePolicy：step-level（train_instance/use_cache）+ epoch-level refresh（离线 rebuild）
  # Gate 6 最小可跑：全部固定 cached + refresh=0
  policy:
    warmup_epochs: 0

    # ---------- step-level (instance) ----------
    instance_mode: "cached"              # ["cached","online","hybrid"]
    instance_update_every_steps: 0
    instance_update_every_epochs: 0
    instance_update_steps: 0

    # ---------- step-level (cheap) ----------
    cheap_mode: "cached"                 # 先保留字段；后续 Gate 7 你再接 cheap online/selector
    cheap_update_every_steps: 0
    cheap_update_every_epochs: 0
    cheap_update_steps: 0

    # ---------- epoch-level refresh (offline rebuild) ----------
    refresh_cheap_cache_every_epochs: 0
    refresh_selection_cache_every_epochs: 0
    refresh_instance_cache_every_epochs: 0

    refresh_selection_follows_cheap: true
    refresh_instance_follows_selection: true

# ===================== 模型：CheapModel（沿用 CheapCTSNet.yaml，供未来 Gate7/离线 refresh 用） =====================
# 注意：Gate 6 最小跑通时你可以不实际用 cheap_model，但字段保留用于完整控制与复用
cheap_model:
  name: CheapCTSNet
  arch: CheapCTSNet_TinyConv
  meta_mode: logit_only
  meta_dropout: 0.2
  emb_dim: 64
  use_diff: true
  dropout: 0.0
  logit_hidden_dim: 64
  c1: 16
  c2: 32
  k1: 5
  k2: 3
  s1: 2
  s2: 2

# ===================== 模型：InstanceModel（沿用 miRAW_TargetNet_Optimized） =====================
# train_em.py 会从以下任一节点取 instance 配置：instance_model / cts_model / model_instance
instance_model:
  name: targetnet_optimized
  arch: TargetNet_Optimized

  num_channels:      [16, 16, 32, 32]
  num_blocks:        [1, 1, 1, 1]
  pool_size:         3
  stem_kernel_size:  5
  block_kernel_size: 3
  skip_connection:   true
  dropout:           0.5

  multi_scale:       false
  se_type:           "basic"
  use_bn:            false
  se_reduction:      8
  target_output_length: 12
  arch_variant:      "opt4_tiny"

# ===================== 模型：Aggregator（沿用 pair_agg_set_transformer baseline） =====================
# train_em.py 用 cfg.model 作为 aggregator config
model:
  arch: PairSetTransformerAggregator
  name: pair_set_transformer_v0

  # IMPORTANT:
  # 默认 in_dim=387 对应 token_provider.assemble 全开：
  #   384(inst_emb) + 1(inst_logit) + 1(esa) + 1(pos) = 387
  # 如果你关掉 esa/pos/logit，需要同步修改 in_dim。
  in_dim: 387

  d_model: 256
  n_heads: 8
  dim_ff: 512
  dropout: 0.1
  ff_activation: gelu

  # Set Transformer encoder
  n_layers: 3
  block_type: isab
  num_inducing_points: 16

  # PMA decoder
  num_seeds: 1
  use_output_sab: false

##########################################################
  # arch: PairTransformerAggregator
  # name: pair_agg_v0

  # # IMPORTANT: in_dim 必须等于 PairLevelDataset.token_dim
  # #   token_dim = emb_dim + 2(+1 if pos_in_token)
  # #   比如 emb_dim=512:
  # #     pos_in_token = true  -> in_dim = 515
  # #     pos_in_token = false -> in_dim = 514
  # # in_dim: 515
  # in_dim: 387     # TargetNet_Optimized embedding dim = 384

  # d_model: 256
  # n_layers: 3
  # n_heads: 8
  # dim_ff: 512
  # dropout: 0.1
  # ff_activation: gelu

  # max_len: 512
  # use_cls_token: true
  # pos_encoding_type: sinusoidal   # 用你的 TransformerEncoder 支持的类型
  # causal: false

  # # 连续位置编码（只在 pos_in_token=false 时真正起作用）
  # use_rel_pos: false              # base: 不用连续位置编码
  # rel_pos_encoding_type: sinusoidal # [mlp, rope, sinusoidal]
  # rel_pos_hidden_dim: 32



# ===================== TrainerEM（EM pipeline 的训练控制） =====================
trainer_em:
  # --- loop control ---
  num_epochs: ${run.num_epochs}
  log_every: 50
  grad_accum_steps: 1
  clip_grad_norm: 0.0
  use_amp: false

  # --- loss（从旧 train: 整体迁移过来）---
  loss_type: focal            # bce / focal
  esa_weighting: true         # 是否使用 ESA 加权（pair-level 如不需要可 false）

  # focal / smoothing / bce 相关
  focal_alpha: 0.4
  focal_gamma: 1.0
  focal_lambda: 1.0

  label_smoothing: true
  smooth_neg: 0.05
  smooth_pos: 0.95

  bce_lambda: 0.01
  pos_weight: 1.0             # 若用于 BCEWithLogitsLoss(pos_weight)
  bce_pos_weight: 1.0         # 兼容你旧代码可能用到的字段名

  # esa weighting（window-size/score weighting）
  esa_scale: 10.0
  esa_lambda_pos: 1.0
  esa_lambda_neg: 0.5

  # --- optimizer（共享超参；agg/inst 只分 lr/wd）---
  optimizer: adamw
  betas: [0.9, 0.999]
  eps: 1e-8
  amsgrad: false

  # 两套 optimizer：agg / inst
  lr_agg: 3e-4
  wd_agg: 1e-2
  lr_inst: 1e-5
  wd_inst: 0.0

  # --- scheduler（对齐 TrainerEMConfig：none/plateau/cosine/step）---
  scheduler_agg: cosine
  scheduler_inst: none
  scheduler_t_max: ${run.num_epochs}
  scheduler_step_size: 10
  scheduler_gamma: 0.1
  scheduler_factor: 0.2
  scheduler_patience: 5

  # --- monitor/best ---
  monitor: loss
  greater_is_better: false

  # --- EMA（默认只对 agg）---
  ema_enabled: true
  ema_decay: 0.999


# ===================== 任务与评估（复用旧 evaluator/metrics 语义） =====================
task:
  problem_type: binary_classification
  from_logits: true
  threshold: 0.5
  ranking:
    enabled: true
    ks: [1, 10, 32, 64, 128]
    compute_topk_overlap: true
    bucket:
      strategy: quantile
      num_buckets: 5

eval:
  save_dir: "eval_outputs"
  do_threshold_sweep: true
  sweep_num_thresholds: 101
  use_val_best_threshold: false
  best_threshold_path: null
  save_metrics_json: true
  save_report_txt: true
  save_threshold_csv: true
  save_curves_png: true
  roc_curve_file: "roc_curve.png"
  pr_curve_file: "pr_curve.png"

# ===================== 日志 logging / wandb =====================
logging:
  wandb:
    enabled: true
    project: "miRAW_EM_Pipeline"
    entity: null
    mode: "online"
    group: "em_pipeline"
    tags: ["EM", "PairAgg", "SetTransformer"]
  eval:
    use_wandb: false
    wandb_prefix: "eval"
