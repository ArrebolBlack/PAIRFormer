# @package _global_



# ---- 顶层元信息（推荐有） ----
experiment_name: "DeepMirTar_Transformer"  # 实验名，用于 WandB / 日志前缀
experiment:
  name: DeepMirTar_Transformer
  task: window_level_train    

seed: 2020                                   # 全局随机种子 
device: "cuda"                               # "cuda" / "cpu"

# ============================================================
# 1. 数据配置 data
# ============================================================
data:
  # 数据集名字（纯标识，用于日志）
  name: mirna_DeepMirTar

  # 原始 txt 路径（相对于项目根目录）
  path:
    train: "data/DeepMirTar/train_seed_1234.txt"
    val:   "data/DeepMirTar/valid_seed_1234.txt"
    test:  "data/DeepMirTar/test_seed_1234.txt"

  # 是否包含 ESA 特征（保持与你现有 DataConfig 逻辑一致）
  with_esa: true

# ============================================================
# 2. 模型配置 model
# ============================================================
model:
  arch: TargetNetTransformer1D         # 使用新注册的模型
  name: targetnet_transformer_baseline

  # === Transformer 结构超参（从你之前的 MyTransformer baseline 迁移而来） ===
  d_model: 128
  n_layers: 2
  n_heads: 4
  dim_ff: 512
  dropout: 0.1

  # 位置编码 & 序列长度相关
  max_len: 64               # 不写则默认 40/50，这里显式给个稍宽裕的
  ff_activation: swiglu       #gelu / relu / swiglu 对应 TransformerConfig.ff_activation
  pos_encoding_type: sinusoidal   # ["none", "sinusoidal", "rope"]
  causal: false             # encoder，用不到因果 mask

  # 可选：如果想明确定义（否则默认 True）
  # with_esa: true
# ============================================================
# 3. 训练配置 train（Trainer 用）
# ============================================================
train:
  # --- 优化器 & 学习率 ---
  optimizer: adamw              # ["adamw", "adam", "sgd", "rmsprop"]
  lr: 0.001
  weight_decay: 0.0
  momentum: 0.9                # 仅对 SGD 有效

  # --- 学习率调度器 ---
  scheduler: cosine              # ["none", "plateau", "cosine", "step"]
  scheduler_factor: 0.2        # for ReduceLROnPlateau
  scheduler_patience: 5
  scheduler_t_max: 100          # for CosineAnnealingLR
  scheduler_step_size: 10      # for StepLR
  scheduler_gamma: 0.1

  # --- 损失 & 数值稳定 ---
  # === Stage 0.1: class weighting + label smoothing ===
  pos_weight: 1.0       # 1.0 = 不加权
  label_smoothing: true
  smooth_pos: 0.95       # label_smoothing=false 时不生效
  smooth_neg: 0.05
  # === Stage 0.2: Focal Loss ===
  loss_type: bce_focal          # ["bce", "focal", "bce_focal"]
  focal_gamma: 1.0
  focal_alpha: 0.4
  focal_lambda: 1
  bce_lambda: 0.01 # bce_loss 数值一般在focal_loss的十倍
  # bce_focal = bce_lambda * bce_loss + focal_lambda * focal_loss
  # === Stage 0.3: esa weight(window-size) ===
  esa_weighting: true
  esa_scale: 10.0     # ESA 归一化尺度，esa_norm = clamp(score / esa_scale, 0..1)
  esa_lambda_pos: 1.0 # 正样本 ESA 权重系数
  esa_lambda_neg: 0.5 # 负样本 ESA 权重系数（一般从 0 或 0.5 小试）
  



  amp: true                    # 是否使用混合精度
  grad_clip: 1.0               # None 或 float

  # --- EMA（可选） ---
  ema:
    enabled: true
    decay: 0.999

  # --- 监控指标（决定 best.pt 保存逻辑） ---
  monitor: loss                # "loss" / "f1" / "roc_auc" / ...
  greater_is_better: false     # 对 loss 应设为 false，对 F1/AUC 设为 true


# ============================================================
# 4. 任务配置 task（metrics / evaluator 用）
# ============================================================
task:
  # 问题类型：影响损失与 metrics 行为
  problem_type: binary_classification   # or "regression"

  # y_pred_raw 是否为 logits，需要内部做 sigmoid
  from_logits: true

  # 最简单版本：固定阈值
  # （之后可以扩展为 threshold.fixed / sweep 等更复杂结构）
  threshold: 0.5

# ============================================================
# 5. 运行配置 run（train.py / eval.py 通用）
# ============================================================
run:
  # 当前运行模式（主要语义标记）
  mode: train                       # "train" / "eval"

  
  num_epochs: 60
  batch_size: 64

  num_workers: 16
  pin_memory: true

  # 数据缓存路径（通常相对于项目根目录）
  cache_path: "${paths.cache_root}"

  # checkpoint / resume 策略
  resume: false                     # train: 是否尝试从 checkpoint 恢复
  checkpoint: null                  # train 默认 None；eval 模式下必须显式指定

  # 输出子目录（相对于 hydra.run.dir）
  ckpt_subdir: "checkpoints"
  eval_subdir: "eval"
  
  eval_test_after_train: true
  test_splits: ["test"]

  reduction: "softmax"          # 统一控制
  train_reduction: ${run.reduction}
  eval_reduction:  ${run.reduction}
  test_reduction:  ${run.reduction}

  train_softmax_temp: 1.0
  eval_softmax_temp: 1.0
  test_softmax_temp: 1.0

  train_topk: 3
  eval_topk: 3
  test_topk: 3

# ============================================================
# 6. 评估配置 eval（evaluator 使用）
# ============================================================
eval:
  # 顶层输出目录名（一般不用改，实际路径= run.eval_subdir / split_idx）
  save_dir: "eval_outputs"

  # 是否在当前 split 上扫阈值（训练结束的 val eval 就会用到）
  do_threshold_sweep: true
  sweep_num_thresholds: 101    # 扫描 0~1 上的阈值个数

  # 是否从 val 阶段的 best_threshold.json 读阈值（主要给 test eval 用）
  use_val_best_threshold: false
  best_threshold_path: null    # 为空时默认尝试 eval/val/best_threshold.json

  # 输出内容控制
  save_metrics_json: true
  save_report_txt: true
  save_threshold_csv: true
  save_curves_png: true

  # 曲线文件名（放在对应 split 目录下）
  roc_curve_file: "roc_curve.png"
  pr_curve_file: "pr_curve.png"

# ============================================================
# 7. 日志 & WandB 配置 logging
# ============================================================
logging:
  # 训练 & eval 统一使用的 WandB 基本设置
  wandb:
    enabled: true                  # 打开后 train/eval 都会 init wandb
    project: "TargetNet"
    entity: null
    mode: "online"                  # "online" / "offline" / "disabled"
    group: "DeepMirTar_Transformer"
    tags: ["Transformer", "DeepMirTar", "Stage-1"]

  # evaluator 内部使用的前缀设置（写 summary 时可以加上 eval/）
  eval:
    use_wandb: false                # 是否在 evaluator 中额外写 wandb summary
    wandb_prefix: "eval"

# ============================================================
# 8. 路径命名空间 paths（统一收口）
# ============================================================
paths:
  # 所有实验输出的根目录（通常配合 Hydra 的 hydra.run.dir 一起用）
  output_root: "outputs"

  # 数据 cache 根目录
  cache_root: "cache"

  # 日志根目录（给以后 logger / 自定义文件日志预留）
  logs_root: "logs"
