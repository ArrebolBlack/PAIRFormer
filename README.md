Project Architecture Overview

é€šç”¨æ·±åº¦å­¦ä¹ æ¨¡æ¿ï¼ˆCNN / Transformer / DiT å‡å¯æŒ‚æ¥ï¼‰ï¼ŒåŸºäº Hydra + OmegaConfã€‚

æ ¸å¿ƒè®¾è®¡ï¼š
æ‰€æœ‰ä¸œè¥¿éƒ½é€šè¿‡é…ç½®è§£è€¦ï¼Œå…¥å£è„šæœ¬åªè´Ÿè´£ã€Œç»„è£…ã€ï¼ŒçœŸæ­£çš„é€»è¾‘è—åœ¨ data / model / trainer / evaluator è¿™äº›æ¨¡å—é‡Œã€‚

1. é¡¶å±‚ Mental Model

ä¸€è¡Œå‘½ä»¤èƒŒåå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

python -m src.launch.train \
  experiment=experiment_mirna \
  model=targetnet_default \
  data=mirna_miraw


å¤§è‡´æµç¨‹ï¼š

Hydra è¯»å– configs/config.yaml å’Œæ‰€æœ‰ defaults groupï¼Œåˆæˆä¸€ä¸ªå¤§ cfg

cfg.data â†’ DataConfig â†’ build_dataset_and_loader â†’ DataLoader

cfg.model â†’ build_model â†’ nn.Module

cfg.train + cfg.task â†’ Trainer

cfg.run + cfg.paths â†’ ç›®å½•ç»“æ„ï¼ˆcheckpoints / eval / cacheï¼‰

cfg.logging â†’ wandb / log è¡Œä¸º

Trainer.train_one_epoch / validate_one_epochï¼Œæœ€å evaluate_with_trainer åšå®Œæ•´è¯„ä¼°

2. Config ä¾èµ–å…³ç³»å›¾

é…ç½® groupï¼š

data

model

train

task

run

eval

logging

paths

å…¥å£è„šæœ¬ï¼š

src/launch/train.py

src/launch/eval.py

2.1 æ€»ä½“ä¾èµ–å›¾ï¼ˆæ–‡æœ¬ç‰ˆï¼‰
                    +----------------------+
                    |     configs/*.yaml   |
                    |  (Hydra + OmegaConf) |
                    +----------+-----------+
                               |
                               v
                        DictConfig cfg
                               |
    +---------------------------+-----------------------------------+
    |                           |                                   |
    v                           v                                   v
 cfg.data                 cfg.model                           cfg.paths
   |                         |                                   |
   v                         v                                   |
DataConfig           ModelConfig(å¯é€‰)/DictConfig                |
   |                         |                                   |
   |                         v                                   |
   |                 models.registry.build_model                 |
   |                                                             |
   v                                                             v
build_dataset_and_loader                                 output_root / cache_root / logs_root
   |                                                             |
   v                                                             |
DataLoader(train/val/test)                                      cfg.run
   |                                                             |
   +---------------------------+---------------------------------+
                               v
                            cfg.train   +  cfg.task   +   cfg.logging
                               |                |              |
                               v                v              |
                          Trainer(model, train_cfg, task_cfg, logger/wandb)
                               |
                               v
        -----------------------------------------------------
        |                  è®­ç»ƒ & éªŒè¯ & è¯„æµ‹               |
        |  - train_one_epoch                              |
        |  - validate_one_epoch (è°ƒç”¨ compute_metrics)     |
        |  - evaluator.evaluate_with_trainer              |
        -----------------------------------------------------
                               |
                               v
                    checkpoints / eval ç»“æœ / wandb

2.2 å„ group è°åœ¨ç”¨ï¼Ÿ

cfg.data

åœ¨ train.py / eval.py ä¸­ï¼š

DataConfig.from_omegaconf(cfg.data)

build_dataset_and_loader(data_cfg, ...)

DataConfig å†³å®šï¼š

åŸå§‹ txt è·¯å¾„ï¼ˆpath.train / path.val / path.test*ï¼‰

æ˜¯å¦å¸¦ ESA ç‰¹å¾ç­‰

cfg.model

åœ¨ train.py / eval.py ä¸­ï¼š

model_name = cfg.model.get("arch", cfg.model.get("name"))

build_model(model_name, cfg.model, data_cfg=data_cfg)

æ¨¡å‹å†…éƒ¨å¯ä»¥æ‹¿ cfg.model æˆ– ModelConfig é‡Œçš„ params åšç»“æ„è¶…å‚ï¼ˆé€šé“æ•°ã€å±‚æ•°ã€dropout ç­‰ï¼‰

cfg.train

åªåœ¨ Trainer é‡Œè½åœ°ï¼š

ä¼˜åŒ–å™¨ï¼šoptimizer, lr, weight_decay, momentum

è°ƒåº¦å™¨ï¼šscheduler, scheduler_factor, scheduler_patience, scheduler_t_max, scheduler_step_size, scheduler_gamma

lossï¼šloss_type (bce / mse / custom)

è®­ç»ƒæŠ€å·§ï¼šamp, grad_clip, ema.enabled, ema.decay

ç›‘æ§æŒ‡æ ‡ï¼šmonitor, greater_is_better

cfg.task

åœ¨ Trainer / compute_metrics / evaluator ä¸­ä½¿ç”¨ï¼š

problem_type: "binary_classification" / "regression"

from_logits: æ˜¯å¦éœ€è¦åœ¨ metrics å†…éƒ¨è‡ªåŠ¨åŠ  sigmoid

threshold: å›ºå®šé˜ˆå€¼æˆ–è€…ï¼ˆæ‰©å±•åï¼‰æ›´å¤æ‚çš„ threshold é…ç½®

compute_metrics(y_true, y_pred_raw, task_cfg) å†…éƒ¨æ ¹æ® problem_type + threshold è®¡ç®— F1 / AUC ç­‰

cfg.run

åœ¨ train.py ä¸­ï¼š

num_epochs

batch_size, num_workers, pin_memory

cache_pathï¼ˆç»“åˆ paths.cache_root â†’ å†³å®š cache ä½ç½®ï¼‰

resume / checkpoint

ckpt_subdir / eval_subdirï¼ˆå†³å®š run å†…éƒ¨è¾“å‡ºç›®å½•ï¼‰

åœ¨ eval.py ä¸­ï¼š

batch_size, num_workers, pin_memory

cache_path

checkpointï¼ˆå¿…é¡»æŒ‡å®šï¼‰

eval_subdirï¼ˆè¯„ä¼°è¾“å‡ºæ ¹ç›®å½•ï¼‰

cfg.eval

åœ¨ evaluator.evaluate_with_trainer ä¸­ä½¿ç”¨ï¼š

æ˜¯å¦åš threshold sweepï¼šdo_threshold_sweep, sweep_num_thresholds

è¾“å‡ºå†…å®¹ï¼šsave_metrics_json, save_report_txt, save_threshold_csv, save_curves_png

æ–‡ä»¶åæ§åˆ¶ï¼šroc_curve_file, pr_curve_file

åœ¨ eval.py ä¸­ï¼š

use_val_best_threshold, best_threshold_pathï¼š

è‹¥å¯ç”¨ï¼Œä¼šåŠ è½½ best_threshold.jsonï¼Œå¹¶é‡å†™ä¸€ä»½ task_eval_cfg.thresholdï¼Œç”¨å›ºå®šé˜ˆå€¼åœ¨ test ä¸Šè·‘ã€‚

cfg.logging

åœ¨ train.py / eval.py çš„ setup_wandb ä¸­ä½¿ç”¨ï¼š

logging.wandb.enabled, project, entity, mode, group, tags

åœ¨ evaluator ä¸­ï¼š

logging.eval.use_wandb

logging.eval.wandb_prefixï¼ˆä¾‹å¦‚ "eval"ï¼Œå†™å…¥ summary æ—¶åŠ å‰ç¼€ï¼‰

cfg.paths

ç»Ÿä¸€å®šä¹‰è·¯å¾„å‘½åç©ºé—´ï¼š

output_root: å®éªŒè¾“å‡ºæ ¹ï¼ˆé€šå¸¸é…åˆ Hydra hydra.run.dir ç”¨ï¼‰

cache_root: æ•°æ® cache æ ¹ï¼Œé€šå¸¸ç›¸å¯¹äºé¡¹ç›® root

logs_root: æ—¥å¿—æ ¹ï¼ˆå¯é€‰ï¼Œç»™ä»¥å logger ç”¨ï¼‰

cfg.run.cache_path: ${paths.cache_root}ï¼šå¤ç”¨ paths çš„å®šä¹‰

ğŸ’¡ æ¨èå®è·µï¼ˆæœ€åè½åœ°ç‰ˆï¼‰ï¼š
åœ¨ configs/config.yaml ä¸­ä½¿ç”¨ hydra.run.dir: ${paths.output_root}/${now:%Y-%m-%d_%H-%M-%S}ï¼Œ
å†åœ¨ run.default.yaml ä¸­é…ç½® ckpt_subdir: "checkpoints", eval_subdir: "eval"ï¼Œ
è¿™æ ·æ¯ä¸ª run çš„ç»“æ„æ¸…æ™°ç»Ÿä¸€ã€‚

3. ä¸€æ¬¡æ ‡å‡†è®­ç»ƒæµç¨‹ï¼ˆTrainï¼‰
3.1 å‘½ä»¤è¡Œä¾‹å­

æœ€å°ç‰ˆï¼ˆä½¿ç”¨é»˜è®¤ experiment / data / modelï¼‰ï¼š

python -m src.launch.train


æŒ‡å®šå®éªŒæ¨¡æ¿ + æ¨¡å‹ï¼š

python -m src.launch.train \
  experiment=experiment_mirna \
  model=targetnet_default


å¸¸è§ override ç¤ºä¾‹ï¼š

# æ”¹å­¦ä¹ ç‡
python -m src.launch.train train.lr=3e-4

# æ”¹ batch size å’Œ epoch æ•°
python -m src.launch.train run.batch_size=2048 run.num_epochs=50

# æŒ‡å®š cache ç›®å½•ï¼ˆä¾‹å¦‚ SSDï¼‰
python -m src.launch.train run.cache_path=/ssd/cache_mirna

# å¼€å¯/å…³é—­ AMP
python -m src.launch.train train.amp=false

# æ‰“å¼€ WandB
python -m src.launch.train logging.wandb.enabled=true logging.wandb.project=mirna_project

3.2 Train è„šæœ¬å†…éƒ¨æµç¨‹ï¼ˆç®€è¦ï¼‰

src/launch/train.pyï¼š

@hydra.main(...) è¯»å– config

set_seeds(cfg.seed)ï¼Œè®¾ç½®éšæœºç§å­ï¼ˆåœ¨ src/utils.set_seeds ç»Ÿä¸€å®ç°ï¼‰

è§£æ deviceï¼šCPU / CUDA

æ ¹æ® cfg.run.ckpt_subdir / cfg.run.eval_subdir + Path.cwd() åˆ›å»º

run_dir/checkpoints/

run_dir/eval/

setup_wandb(cfg)ï¼šè‹¥å¼€å¯ï¼Œåˆ™ wandb.init(config=cfg)

data_cfg = DataConfig.from_omegaconf(cfg.data)

ä½¿ç”¨ cfg.run.cache_pathï¼ˆç»“åˆ paths.cache_rootï¼‰æ„é€  cache è·¯å¾„ï¼Œå¹¶ä¼ ç»™ build_dataset_and_loader

build_dataset_and_loader æ„å»ºï¼š

train_loaderï¼ˆsplit_idx="train"ï¼‰

val_loaderï¼ˆsplit_idx="val"ï¼‰

val_set_labels = get_set_labels(data_cfg, "val")ï¼šè¯»å– set-level æ ‡ç­¾

model = build_model(model_name, cfg.model, data_cfg=data_cfg)

trainer = Trainer(model, task_cfg=cfg.task, train_cfg=cfg.train, device=device)

å¦‚ cfg.run.resume / cfg.run.checkpoint æ‰“å¼€ï¼Œåˆ™ trainer.load_checkpoint(...)

for epoch in range(trainer.state.epoch, cfg.run.num_epochs):

train_metrics = trainer.train_one_epoch(train_loader)

val_metrics = trainer.validate_one_epoch(val_loader, set_labels=val_set_labels, aggregate_sets=True, use_ema=True)

ä¿å­˜ last.pt / best.pt

è‹¥ wandb æ‰“å¼€ï¼Œè®°å½• train/loss, val/loss, val/f1, val/roc_auc, val/pr_auc ç­‰

è®­ç»ƒç»“æŸåï¼š

ç”¨ evaluate_with_trainer(...) åœ¨ val ä¸Šåšä¸€æ¬¡å®Œæ•´è¯„ä¼°ï¼ˆå« threshold sweep / ROC / PR æ›²çº¿ç­‰ï¼‰

è¾“å‡ºåˆ° run_dir/eval/val

è‹¥ wandb æ‰“å¼€ï¼Œå†™å…¥ wandb.run.summary["val/xxx"]

4. ä¸€æ¬¡æ ‡å‡†è¯„ä¼°æµç¨‹ï¼ˆEvalï¼‰
4.1 æ¨¡å¼ 1ï¼šå›ºå®šé˜ˆå€¼ï¼ˆæ¥è‡ª cfg.task.thresholdï¼‰

æœ€ç®€å•ç‰ˆæœ¬ï¼šç”¨å½“å‰ config çš„ threshold ç›´æ¥è¯„ä¼° test é›†ã€‚

python -m src.launch.eval \
  run.checkpoint=/path/to/best.pt \
  data=mirna_miraw


ç‰¹ç‚¹ï¼š

cfg.task.threshold å†³å®šä½¿ç”¨çš„é˜ˆå€¼ï¼ˆä¾‹å¦‚ 0.5ï¼‰

è‹¥ cfg.eval.do_threshold_sweep=trueï¼Œåˆ™ä¼šåœ¨å½“å‰ split ä¸Šæ‰«ä¸€éï¼ˆå¯é€‰ï¼‰

4.2 æ¨¡å¼ 2ï¼šå¤ç”¨è®­ç»ƒé˜¶æ®µçš„ best_threshold

å‡è®¾ä½ åœ¨è®­ç»ƒæ—¶ï¼Œval è¯„ä¼°é˜¶æ®µç”Ÿæˆäº†ï¼š

<run_dir>/eval/val/best_threshold.json
{
  "best_threshold": 0.7345,
  "monitor": "f1",
  ...
}


è¯„ä¼°è„šæœ¬ä¸­ï¼š

cfg.eval.use_val_best_threshold=true

cfg.eval.best_threshold_path=/abs/path/to/best_threshold.json
ï¼ˆè‹¥ä¸æŒ‡å®šï¼Œé»˜è®¤å°è¯• run_dir/eval/val/best_threshold.jsonï¼‰

å‘½ä»¤ï¼š

python -m src.launch.eval \
  run.checkpoint=/path/to/best.pt \
  eval.use_val_best_threshold=true \
  eval.best_threshold_path=/abs/run_xxx/eval/val/best_threshold.json


å†…éƒ¨è¡Œä¸ºï¼š

è¯»å…¥ best_threshold

å…‹éš†ä¸€ä»½ task_eval_cfg = OmegaConf.create(cfg.task)

æ”¹å†™ï¼š

task_eval_cfg.threshold.value = best_threshold

task_eval_cfg.threshold.fixed = true

task_eval_cfg.threshold.sweep = falseï¼ˆå¦‚æœä½ åç»­æ‰©å±•äº†è¿™ä¸¤ä¸ªå­—æ®µï¼‰

ç”¨è¿™ä»½ task_eval_cfg å¯¹æ‰€æœ‰ data_cfg.path.keys()ï¼ˆé€šå¸¸ ["test"] æˆ– ["test0", "test1", ...]ï¼‰è¿›è¡Œè¯„ä¼°

è¾“å‡ºç»“æœè‡³ï¼š

run_dir/eval/<split_idx>/metrics.json

run_dir/eval/<split_idx>/report.txt

run_dir/eval/<split_idx>/roc_curve.png / pr_curve.png

è‹¥ wandb æ‰“å¼€ï¼Œsummary ä¸­ä¼šæœ‰ï¼š

<split_idx>/f1, <split_idx>/roc_auc, <split_idx>/pr_auc

eval/best_threshold

5. é…ç½®è‡ªæ£€ï¼šä¸€é”®æ‰“å°å…¨éƒ¨é…ç½®

ä½ å·²ç»æœ‰è„šæœ¬ï¼š

python ./scripts/print_all_configs.py


å®ƒä¼šæ‰“å°å½“å‰ç»„åˆåçš„ï¼š

data

model

train

task

run

eval

logging

paths

ç”¨äºå¿«é€Ÿ sanity checkï¼Œæ¯”å¦‚ä½ åˆšåˆšçš„è¾“å‡ºï¼š

==== data ====
name: mirna_miraw
...

==== model ====
name: targetnet_default
arch: TargetNet
...

...


æ¨èä¹ æƒ¯ï¼š

æ¯æ¬¡å¤§æ”¹ config ç»“æ„åï¼Œå…ˆè·‘ä¸€é print_all_configs.py ç¡®è®¤æ²¡æœ‰ keyErrorã€æ²¡æœ‰ typo

æŠŠå…¸å‹çš„å®éªŒç»„åˆï¼ˆæ¯”å¦‚ experiment_mirnaï¼‰å†™åœ¨ README çš„ã€Œå®éªŒæ¸…å•ã€é‡Œï¼Œæœªæ¥è¦å¤ç°æ—¶å°±ç…§ç€ç‚¹èœã€‚



1. å•æ¬¡è®­ç»ƒï¼šæœ€åŸºæœ¬çš„å‘½ä»¤

ä½¿ç”¨é»˜è®¤ config.yaml + é»˜è®¤ experimentï¼š

python -m src.launch.train


æŒ‡å®šä¸€ä¸ªå®éªŒ presetï¼ˆæ¯”å¦‚ configs/experiment/mirna_baseline.yamlï¼‰ï¼š

python -m src.launch.train experiment=mirna_baseline


å¸¸è§çš„è¦†å†™æ–¹å¼ï¼ˆå‘½ä»¤è¡Œä¼˜å…ˆçº§æœ€é«˜ï¼‰ï¼š

python -m src.launch.train \
  experiment=mirna_baseline \
  train.lr=3e-4 \
  train.weight_decay=0.0 \
  run.num_epochs=50 \
  logging.wandb.enabled=true \
  logging.wandb.group=mirna_baseline_v2 \
  logging.wandb.tags="[mirna,baseline,v2]"


è¯´æ˜ï¼š

experiment=mirna_baseline
â†’ ç”¨ configs/experiment/mirna_baseline.yaml è¦†ç›–é»˜è®¤é…ç½®

train.*
â†’ è¦†ç›–è®­ç»ƒè¶…å‚

run.*
â†’ æ§åˆ¶ epoch / batch_size / checkpoint ç­‰

logging.wandb.*
â†’ æ§åˆ¶ WandB é¡¹ç›®ã€åˆ†ç»„å’Œæ ‡ç­¾ï¼ˆä¸ç”¨æ”¹ yaml æ–‡ä»¶ï¼‰

2. è¯„ä¼°è„šæœ¬ï¼šåŠ è½½ checkpoint åšå®Œæ•´è¯„æµ‹

ç»™å®šä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„ checkpointï¼ˆä¾‹å¦‚ï¼šoutputs/exp1/checkpoints/best.ptï¼‰ï¼Œåš test è¯„æµ‹ï¼š

python -m src.launch.eval \
  experiment=mirna_baseline \
  run.checkpoint="outputs/exp1/checkpoints/best.pt"


å¦‚æœæƒ³åœ¨ eval é˜¶æ®µä¹Ÿç”¨ WandB è®°å½•æŒ‡æ ‡ï¼š

python -m src.launch.eval \
  experiment=mirna_baseline \
  run.checkpoint="outputs/exp1/checkpoints/best.pt" \
  logging.wandb.enabled=true \
  logging.wandb.group=mirna_eval_v1 \
  logging.wandb.tags="[mirna,eval]"


è¯´æ˜ï¼š

run.checkpoint å¿…é¡»æ˜¾å¼æŒ‡å®š

eval è„šæœ¬å†…éƒ¨ä¼šéå† data.path ä¸‹æ‰€æœ‰ splitï¼ˆä¾‹å¦‚ test, test0, test1ï¼‰ï¼Œåˆ†åˆ«è¯„ä¼°

è¯„ä¼°ç»“æœä¼šå†™åˆ° ${run.eval_subdir}ï¼ˆé»˜è®¤ outputs/eval ä¹‹ç±»ï¼‰

3. å¤šä»»åŠ¡ / å¤šè¶…å‚æœç´¢ï¼šHydra multirun (-m)

Hydra åŸç”Ÿæ”¯æŒå¤šç»„å‚æ•°ä¸€é”®è·‘ï¼Œå…¸å‹ç”¨æ³•æ˜¯åŠ  -mï¼ˆmultirunï¼‰ã€‚

3.1 å¯¹ä¸€ä¸ªå®éªŒåšè¶…å‚ç½‘æ ¼æœç´¢

ä¾‹å¦‚ï¼šåœ¨ baseline ä¸Šåš (lr, batch_size) ç¬›å¡å°”ç§¯æœç´¢ï¼š

python -m src.launch.train -m \
  experiment=mirna_baseline \
  train.lr=1e-3,3e-4,1e-4 \
  run.batch_size=512,1024


Hydra ä¼šè‡ªåŠ¨å±•å¼€æˆ 3 Ã— 2 = 6 ä¸ª runï¼Œæ¯ä¸ª run æœ‰è‡ªå·±ç‹¬ç«‹çš„ hydra.run.dirï¼Œä¾‹å¦‚ï¼š

multirun/2025-11-27/00-00-00/0

multirun/2025-11-27/00-00-00/1

â€¦

æ¯ä¸ªç›®å½•ä¸‹éƒ½æœ‰ï¼š

è¯¥æ¬¡ run çš„é…ç½®å¿«ç…§ï¼š.hydra/config.yaml

ä½ çš„è¾“å‡ºï¼šoutputs/checkpointsã€outputs/eval ç­‰

ä½ å¯ä»¥è¿›ä¸€æ­¥æŒ‡å®š multi-run çš„è¾“å‡ºæ ¹ç›®å½•ï¼š

python -m src.launch.train -m \
  hydra.sweep.dir="multirun/miRNA_lr_bs_sweep" \
  experiment=mirna_baseline \
  train.lr=1e-3,3e-4,1e-4 \
  run.batch_size=512,1024

3.2 å¤šä¸ª experiment ä¸€æ¬¡æ€§è·‘å®Œ

æ¯”å¦‚ä½ æœ‰ä¸¤ä¸ª presetï¼š

experiment=mirna_baseline

experiment=sirna_baseline

å¯ä»¥è¿™æ ·ä¸€æ¬¡è·‘ä¸¤ä¸ªï¼š

python -m src.launch.train -m \
  experiment=mirna_baseline,sirna_baseline \
  train.lr=1e-3 \
  run.num_epochs=30


Hydra ä¼šç”Ÿæˆä¸¤æ¡ runï¼Œåˆ†åˆ«å¯¹åº”ä¸åŒ experiment çš„é…ç½®ã€‚

3.3 é…åˆ WandB çš„åˆ†ç»„ç­–ç•¥ï¼ˆæ¨èå®è·µï¼‰

å¸¸è§æ¨¡å¼æ˜¯ï¼š

æ¯ä¸€æ¡ multirun çš„å®éªŒï¼ŒWandB ç”¨åŒä¸€ä¸ª groupï¼›

æ¯ä¸ªå­ run è‡ªåŠ¨å¸¦è‡ªå·±çš„ hydra.job.numï¼ˆä½ å¯ä»¥è‡ªå·±åŠ åˆ° tags/name é‡Œï¼‰ã€‚

ç¤ºä¾‹ï¼š

python -m src.launch.train -m \
  experiment=mirna_baseline \
  logging.wandb.enabled=true \
  logging.wandb.project=targetnet-refactor \
  logging.wandb.entity=myuser \
  logging.wandb.group="mirna_lr_bs_sweep" \
  train.lr=1e-3,3e-4,1e-4 \
  run.batch_size=512,1024


ç„¶ååœ¨ WandB é¢æ¿ä¸­æŒ‰ group = mirna_lr_bs_sweep è¿‡æ»¤ï¼Œå°±èƒ½çœ‹åˆ°è¿™ä¸€ç»„è¶…å‚æœç´¢çš„æ‰€æœ‰ trackã€‚

å¦‚æœä½ ä¹‹åæƒ³æŠŠ Hydra çš„ hydra.job.num æ‹¼è¿› run nameï¼Œå¯ä»¥åœ¨ä»£ç é‡Œç”¨ï¼š

job_num = cfg.hydra.job.num  # int
run_name = f"{cfg.experiment_name}_job{job_num}"